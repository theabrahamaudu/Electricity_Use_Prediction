{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "version = \"0.1.0\"\n",
    "serie_size =  X_train.shape[1]\n",
    "n_features =  X_train.shape[2]\n",
    "\n",
    "epochs = 20\n",
    "batch = 16\n",
    "lr = 0.0001\n",
    "\n",
    "# Build the model\n",
    "encoder_decoder = Sequential()\n",
    "encoder_decoder.add(L.LSTM(serie_size, activation='tanh', input_shape=(serie_size, n_features), return_sequences=True))\n",
    "encoder_decoder.add(L.LSTM(32, activation='tanh', return_sequences=True))\n",
    "encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=False))\n",
    "encoder_decoder.add(L.RepeatVector(serie_size))\n",
    "encoder_decoder.add(L.LSTM(serie_size, activation='tanh', return_sequences=True))\n",
    "encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=False))\n",
    "encoder_decoder.add(L.Dense(1))\n",
    "encoder_decoder.summary()\n",
    "\n",
    "adam = optimizers.Adam(lr)\n",
    "encoder_decoder.compile(loss='mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "version = \"0.2.1\"\n",
    "serie_size =  X_train.shape[1]\n",
    "n_features =  X_train.shape[2]\n",
    "\n",
    "epochs = 50\n",
    "batch = 16\n",
    "lr = 0.0001\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, BatchNormalization\n",
    "\n",
    "# Build the improved model\n",
    "improved_encoder_decoder = Sequential()\n",
    "improved_encoder_decoder.add(Bidirectional(L.LSTM(serie_size, activation='tanh', input_shape=(serie_size, n_features), return_sequences=True)))\n",
    "improved_encoder_decoder.add(L.LSTM(64, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(32, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=False))\n",
    "improved_encoder_decoder.add(BatchNormalization())\n",
    "improved_encoder_decoder.add(L.RepeatVector(serie_size))\n",
    "improved_encoder_decoder.add(L.LSTM(serie_size, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(BatchNormalization())\n",
    "improved_encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(32, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(Bidirectional(L.LSTM(64, activation='tanh', return_sequences=False)))\n",
    "improved_encoder_decoder.add(Dropout(0.2))\n",
    "improved_encoder_decoder.add(L.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "adam = optimizers.Adam(lr)\n",
    "improved_encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "# Print model summary\n",
    "improved_encoder_decoder.build(input_shape=(None, serie_size, n_features))\n",
    "encoder_decoder = improved_encoder_decoder\n",
    "encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "version = \"0.2.2\"\n",
    "serie_size =  X_train.shape[1]\n",
    "n_features =  X_train.shape[2]\n",
    "\n",
    "epochs = 100\n",
    "batch = 16\n",
    "lr = 0.00001\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, BatchNormalization\n",
    "\n",
    "# Build the improved model\n",
    "improved_encoder_decoder = Sequential()\n",
    "improved_encoder_decoder.add(Bidirectional(L.LSTM(serie_size, activation='tanh', input_shape=(serie_size, n_features), return_sequences=True)))\n",
    "improved_encoder_decoder.add(L.LSTM(128, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(64, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=False))\n",
    "improved_encoder_decoder.add(BatchNormalization())\n",
    "improved_encoder_decoder.add(L.RepeatVector(serie_size))\n",
    "improved_encoder_decoder.add(L.LSTM(serie_size, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(BatchNormalization())\n",
    "improved_encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(64, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(128, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(Bidirectional(L.LSTM(128, activation='tanh', return_sequences=False)))\n",
    "improved_encoder_decoder.add(Dropout(0.2))\n",
    "improved_encoder_decoder.add(L.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "adam = optimizers.Adam(lr)\n",
    "improved_encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "# Print model summary\n",
    "improved_encoder_decoder.build(input_shape=(None, serie_size, n_features))\n",
    "encoder_decoder = improved_encoder_decoder\n",
    "encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "version = \"0.3.0\"\n",
    "serie_size =  X_train.shape[1]\n",
    "n_features =  X_train.shape[2]\n",
    "\n",
    "epochs = 100\n",
    "batch = 16\n",
    "lr = 0.00001\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, BatchNormalization\n",
    "\n",
    "# Build the improved model\n",
    "improved_encoder_decoder = Sequential()\n",
    "improved_encoder_decoder.add(Bidirectional(L.LSTM(serie_size, activation='tanh', input_shape=(serie_size, n_features), return_sequences=True)))\n",
    "improved_encoder_decoder.add(L.LSTM(256, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(128, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(64, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=False))\n",
    "improved_encoder_decoder.add(BatchNormalization())\n",
    "improved_encoder_decoder.add(L.RepeatVector(serie_size))\n",
    "improved_encoder_decoder.add(L.LSTM(serie_size, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(BatchNormalization())\n",
    "improved_encoder_decoder.add(L.LSTM(16, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(64, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(128, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(L.LSTM(256, activation='tanh', return_sequences=True))\n",
    "improved_encoder_decoder.add(Bidirectional(L.LSTM(256, activation='tanh', return_sequences=False)))\n",
    "improved_encoder_decoder.add(Dropout(0.2))\n",
    "improved_encoder_decoder.add(L.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "adam = optimizers.Adam(lr)\n",
    "improved_encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "# Print model summary\n",
    "improved_encoder_decoder.build(input_shape=(None, serie_size, n_features))\n",
    "encoder_decoder = improved_encoder_decoder\n",
    "encoder_decoder.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
